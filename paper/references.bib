@misc{tiktoken,
    author={{OpenAI}},
    title={tiktoken},
    url={{https://github.com/openai/tiktoken/tree/main}},
    year={2024}}}

@misc{githubChatgpt_system_promptpromptsgpt4v_bingmdMain,
	author = {LouisShark},
	title = {chatgpt\_system\_prompt/prompts/gpt4v\_bing.md},
	url = {{https://github.com/LouisShark/chatgpt\_system\_prompt/blob/main/prompts/gpt4v\_bing.md}},
	year = {2024},
	note = {[Accessed 01-01-2024]},
}


@article{grynbaum_times_2023,
	chapter = {Business},
	title = {The {Times} {Sues} {OpenAI} and {Microsoft} {Over} {A}.{I}. {Use} of {Copyrighted} {Work}},
	issn = {0362-4331},
	url = {https://www.nytimes.com/2023/12/27/business/media/new-york-times-open-ai-microsoft-lawsuit.html},
	abstract = {Millions of articles from The New York Times were used to train chatbots that now compete with it, the lawsuit said.},
	language = {en-US},
	urldate = {2024-01-01},
	journal = {The New York Times},
	author = {Grynbaum, Michael M. and Mac, Ryan},
	month = dec,
	year = {2023},
	keywords = {Artificial Intelligence, ChatGPT, Compensation for Damages (Law), Copyrights and Copyright Violations, Microsoft Corp, New York Times, News and News Media, Newspapers, OpenAI Labs, Suits and Litigation (Civil)},
	file = {Snapshot:/home/josh/Zotero/storage/W79VKTFV/new-york-times-open-ai-microsoft-lawsuit.html:text/html},
}

@misc{llmlitigationLitigationJoseph,
	author = {},
	title = {{L}{L}{M} litigation · {J}oseph {S}averi {L}aw {F}irm \& {M}atthew {B}utterick --- llmlitigation.com},
	howpublished = {\url{https://llmlitigation.com/}},
	year = {},
	note = {[Accessed 01-01-2024]},
}

@article{chiang2023chatgpt,
  title={ChatGPT is a blurry JPEG of the web},
  author={Chiang, Ted},
  journal={The New Yorker},
  volume={9},
  pages={2023},
  year={2023}
}

@article{mccoy2023much,
  title={How much do language models copy from their training data? evaluating linguistic novelty in text generation using raven},
  author={McCoy, R Thomas and Smolensky, Paul and Linzen, Tal and Gao, Jianfeng and Celikyilmaz, Asli},
  journal={Transactions of the Association for Computational Linguistics},
  volume={11},
  pages={652--670},
  year={2023},
  publisher={MIT Press One Broadway, 12th Floor, Cambridge, Massachusetts 02142, USA~…}
}

@misc{carlini2021extracting,
      title={Extracting Training Data from Large Language Models}, 
      author={Nicholas Carlini and Florian Tramer and Eric Wallace and Matthew Jagielski and Ariel Herbert-Voss and Katherine Lee and Adam Roberts and Tom Brown and Dawn Song and Ulfar Erlingsson and Alina Oprea and Colin Raffel},
      year={2021},
      eprint={2012.07805},
      archivePrefix={arXiv},
      primaryClass={cs.CR}
}

@inproceedings{Lee_2023, series={WWW ’23},
   title={Do Language Models Plagiarize?},
   url={http://dx.doi.org/10.1145/3543507.3583199},
   DOI={10.1145/3543507.3583199},
   booktitle={Proceedings of the ACM Web Conference 2023},
   publisher={ACM},
   author={Lee, Jooyoung and Le, Thai and Chen, Jinghui and Lee, Dongwon},
   year={2023},
   month=apr, collection={WWW ’23} }

@misc{weller2023according,
      title={"According to ..." Prompting Language Models Improves Quoting from Pre-Training Data}, 
      author={Orion Weller and Marc Marone and Nathaniel Weir and Dawn Lawrie and Daniel Khashabi and Benjamin Van Durme},
      year={2023},
      eprint={2305.13252},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{carlini2023quantifying,
      title={Quantifying Memorization Across Neural Language Models}, 
      author={Nicholas Carlini and Daphne Ippolito and Matthew Jagielski and Katherine Lee and Florian Tramer and Chiyuan Zhang},
      year={2023},
      eprint={2202.07646},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{yu2023assessing,
      title={Assessing Prompt Injection Risks in 200+ Custom GPTs}, 
      author={Jiahao Yu and Yuhang Wu and Dong Shu and Mingyu Jin and Xinyu Xing},
      year={2023},
      eprint={2311.11538},
      archivePrefix={arXiv},
      primaryClass={cs.CR}
}

@misc{nasr2023scalable,
      title={Scalable Extraction of Training Data from (Production) Language Models}, 
      author={Milad Nasr and Nicholas Carlini and Jonathan Hayase and Matthew Jagielski and A. Feder Cooper and Daphne Ippolito and Christopher A. Choquette-Choo and Eric Wallace and Florian Tramèr and Katherine Lee},
      year={2023},
      eprint={2311.17035},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{noauthor_extracting_2023,
	title = {Extracting {Training} {Data} from {ChatGPT}},
	url = {https://not-just-memorization.github.io/extracting-training-data-from-chatgpt.html},
	urldate = {2024-01-05},
	month = nov,
	year = {2023},
	file = {Snapshot:/Users/ChrisJimenez/Zotero/storage/QNJVZ542/extracting-training-data-from-chatgpt.html:text/html},
}


@misc{githubChatgpt_system_promptpromptsgpt4v_bingmdMain,
	author = {LouisShark},
	title = {chatgpt\_system\_prompt/prompts/gpt4v\_bing.md},
	url = {{https://github.com/LouisShark/chatgpt\_system\_prompt/blob/main/prompts/gpt4v\_bing.md}},
	year = {2024},
	note = {[Accessed 2024-01-01]},
}

@article{sudmann2018media,
  title={On the media-political dimension of artificial intelligence: Deep learning as a black box and OpenAI},
  author={Sudmann, Andreas},
  journal={Digital Culture \& Society},
  volume={4},
  number={1},
  pages={181--200},
  year={2018},
  publisher={transcript Verlag}
}

@misc{openaiOpenAIJournalism,
	author = {{OpenAI}},
	title = {{O}pen{A}{I} and journalism},
	url = {{https://openai.com/blog/openai-and-journalism}},
	year = {2024},
	note = {[Accessed 2024-01-11]},
}

@article{grynbaum_times_2023,
	chapter = {Business},
	title = {The {Times} {Sues} {OpenAI} and {Microsoft} {Over} {A}.{I}. {Use} of {Copyrighted} {Work}},
	issn = {0362-4331},
	url = {https://www.nytimes.com/2023/12/27/business/media/new-york-times-open-ai-microsoft-lawsuit.html},
	abstract = {Millions of articles from The New York Times were used to train chatbots that now compete with it, the lawsuit said.},
	language = {en-US},
	note = {[Accessed 2024-01-01]},
	journal = {The New York Times},
	author = {Grynbaum, Michael M. and Mac, Ryan},
	month = dec,
	year = {2023},
	keywords = {Artificial Intelligence, ChatGPT, Compensation for Damages (Law), Copyrights and Copyright Violations, Microsoft Corp, New York Times, News and News Media, Newspapers, OpenAI Labs, Suits and Litigation (Civil)},
	file = {Snapshot:/home/josh/Zotero/storage/W79VKTFV/new-york-times-open-ai-microsoft-lawsuit.html:text/html},
}

@misc{llmlitigationLitigationJoseph,
	author = {},
	title = {{L}{L}{M} litigation · {J}oseph {S}averi {L}aw {F}irm \& {M}atthew {B}utterick},
	url = {{https://llmlitigation.com/}},
	year = {2023},
	note = {[Accessed 2024-01-01]},
}

@article{chiang2023chatgpt,
  title={ChatGPT is a blurry JPEG of the web},
  author={Chiang, Ted},
  journal={The New Yorker},
  volume={9},
  pages={2023},
  year={2023}
}

@article{mccoy2023much,
  title={How much do language models copy from their training data? evaluating linguistic novelty in text generation using raven},
  author={McCoy, R Thomas and Smolensky, Paul and Linzen, Tal and Gao, Jianfeng and Celikyilmaz, Asli},
  journal={Transactions of the Association for Computational Linguistics},
  volume={11},
  pages={652--670},
  year={2023},
  publisher={MIT Press One Broadway, 12th Floor, Cambridge, Massachusetts 02142, USA~…}
}

@misc{carlini2021extracting,
      title={Extracting Training Data from Large Language Models}, 
      author={Nicholas Carlini and Florian Tramer and Eric Wallace and Matthew Jagielski and Ariel Herbert-Voss and Katherine Lee and Adam Roberts and Tom Brown and Dawn Song and Ulfar Erlingsson and Alina Oprea and Colin Raffel},
      year={2021},
      eprint={2012.07805},
      archivePrefix={arXiv},
      primaryClass={cs.CR}
}

@inproceedings{Lee_2023, series={WWW ’23},
   title={Do Language Models Plagiarize?},
   url={http://dx.doi.org/10.1145/3543507.3583199},
   DOI={10.1145/3543507.3583199},
   booktitle={Proceedings of the ACM Web Conference 2023},
   publisher={ACM},
   author={Lee, Jooyoung and Le, Thai and Chen, Jinghui and Lee, Dongwon},
   year={2023},
   month=apr, collection={WWW ’23} }

@misc{weller2023according,
      title={``{A}ccording to ...'' Prompting Language Models Improves Quoting from Pre-Training Data}, 
      author={Orion Weller and Marc Marone and Nathaniel Weir and Dawn Lawrie and Daniel Khashabi and Benjamin Van Durme},
      year={2023},
      eprint={2305.13252},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{carlini2023quantifying,
      title={Quantifying Memorization Across Neural Language Models}, 
      author={Nicholas Carlini and Daphne Ippolito and Matthew Jagielski and Katherine Lee and Florian Tramer and Chiyuan Zhang},
      year={2023},
      eprint={2202.07646},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{yu2023assessing,
      title={Assessing Prompt Injection Risks in 200+ Custom GPTs}, 
      author={Jiahao Yu and Yuhang Wu and Dong Shu and Mingyu Jin and Xinyu Xing},
      year={2023},
      eprint={2311.11538},
      archivePrefix={arXiv},
      primaryClass={cs.CR}
}

@misc{nasr2023scalable,
      title={Scalable Extraction of Training Data from (Production) Language Models}, 
      author={Milad Nasr and Nicholas Carlini and Jonathan Hayase and Matthew Jagielski and A. Feder Cooper and Daphne Ippolito and Christopher A. Choquette-Choo and Eric Wallace and Florian Tramèr and Katherine Lee},
      year={2023},
      eprint={2311.17035},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{noauthor_extracting_2023,
	title = {Extracting {Training} {Data} from {ChatGPT}},
	url = {https://not-just-memorization.github.io/extracting-training-data-from-chatgpt.html},
	note = {[Access 2024-01-05]},
	month = nov,
	year = {2023},
	file = {Snapshot:/Users/ChrisJimenez/Zotero/storage/QNJVZ542/extracting-training-data-from-chatgpt.html:text/html},
}

@article{textgened2023,
Vee, A., Laquintano, T., & Schnitzler, C. (Eds.) (2023). TextGenEd: Teaching with Text Generation Technologies. The WAC Clearinghouse. https://doi.org/10.37514/TWR-J.2023.1.1.02

https://wac.colostate.edu/repository/collections/textgened/

}